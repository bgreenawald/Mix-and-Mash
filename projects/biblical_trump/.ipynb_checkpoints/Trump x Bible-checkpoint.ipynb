{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Trump tweets and Bible Verses in a Markov Model and generate random text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import normalize\n",
    "import pickle\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the bible\n",
    "with open(\"bible.txt\", \"r\") as bible:\n",
    "    bible_text = bible.read()\n",
    "    bible.close()\n",
    "    \n",
    "# Regular expression to consolidate verses into single lines\n",
    "single_line = re.compile(\"\\n(?!\\n)\")\n",
    "bible_text = re.sub(pattern=single_line, string=bible_text, repl=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bgree\\Miniconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (15,30,31,32,36,37,38,42,43,44,45,46,47,50,51,52,56,57,58,59,60,61,62,63,64,65,66,67) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Read in the trump tweets\n",
    "tweets = pd.read_csv(\"trumpTweets.csv\")\n",
    "tweets_text = tweets[\"text\"]\n",
    "del tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append tweets to Bible\n",
    "bible_and_tweets = bible_text + \"\\n\".join(tweets_text)\n",
    "with open(\"bible_and_tweets.txt\", \"w+\", encoding=\"utf-8\") as file:\n",
    "    file.write(bible_and_tweets)\n",
    "    file.close()\n",
    "del bible_text, bible_and_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back in the results\n",
    "with open(\"bible_and_tweets.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    bible_and_tweets = file.read()\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regular expression to edit data\n",
    "remove_non_alphabetic = re.compile(\"[^a-zA-Z \\n]\")\n",
    "remove_multiple_newlines = re.compile(\"\\n[\\n]+\")\n",
    "remove_multiple_spaces = re.compile(\" [ ]+\")\n",
    "remove_http = re.compile(\"http[^ ]+\")\n",
    "bible_and_tweets = re.sub(pattern=remove_non_alphabetic, string=bible_and_tweets, repl=\"\")\n",
    "bible_and_tweets = re.sub(pattern=remove_multiple_newlines, string=bible_and_tweets, repl=\"\\n\")\n",
    "bible_and_tweets = re.sub(pattern=remove_http, string=bible_and_tweets, repl=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_and_tweets = bible_and_tweets.replace(\"\\n\", \" ENDLINE\\n\")\n",
    "bible_and_tweets = re.sub(pattern=remove_multiple_spaces, string=bible_and_tweets, repl=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the old testament of the king james version of the bible endline\n",
      " the first book of moses called genesis endline\n",
      " in the beginning god created the heavens and the earth endline\n",
      " and the earth was without form and void and darkness was upon the face of the deep and the spirit of god moved upon the face of the waters endline\n",
      " and god said let there be light and there was light endline\n",
      " and god saw the light that it was good and god divided the light from the darkness endline\n",
      " and god called the light day and the darkness he called night and the evening and the morning were the first day endline\n",
      " and god said let there be a firmament in the midst of the waters and let it divide the waters from the waters endline\n",
      " and god made the firmament and divided the waters which were under the firmament from the waters which were above the firmament and it was so endline\n",
      " and god called the firmament heaven and the evening and the morning were the second day endline\n",
      " and god said let the waters unde\n",
      "d trace not penn are you drugged endline\n",
      "dennisrodman apprenticenbc dennisyou were greatthanks endline\n",
      "christinefox see you later endline\n",
      "vanindc dannyzuker thanks vanso true endline\n",
      "jilltoma hi amy see you soon endline\n",
      "shock obama wh given three pinocchios for lying about benghazi emails it amazing that obama never knew about the irs scandals until he saw it in the news endline\n",
      "very sad that republican donors were targeted by obamas irs endline\n",
      "one season ends and another starts already casting for the next apprenticenbc great news for charity million so far endline\n",
      "emtgonenutz thanks tanja endline\n",
      "the lincoln day dinner last night in michigan was fantastic record attendance and tremendous enthusiasm i loved it endline\n",
      " circulation is way down and all he thinks about are his bad food restaurants condenastcorp endline\n",
      "i cant believe vanityfair would renew graydon carters contract endline\n",
      "graydon carter is laughing at the stupidity of chuck townsend on his contract renewal even he doesn\n"
     ]
    }
   ],
   "source": [
    "bible_and_tweets = bible_and_tweets.lower()\n",
    "print(bible_and_tweets[0:1000])\n",
    "print(bible_and_tweets[5000000:5001000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim each line, write result back to file\n",
    "sentences = []\n",
    "for line in bible_and_tweets.split(\"endline\\n\"):\n",
    "    sentences.append(line.strip())\n",
    "\n",
    "with open(\"text_processed.txt\", \"w+\") as file:\n",
    "    file.write(\" endline\\n\".join(sentences))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocabulary and transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7598559\n"
     ]
    }
   ],
   "source": [
    "# Read in the full text\n",
    "with open(\"text_processed.txt\", \"r\") as file:\n",
    "    text_full = file.read()\n",
    "    file.close()\n",
    "print(len(text_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1366285\n"
     ]
    }
   ],
   "source": [
    "# Create the vocabulary\n",
    "vocab = []\n",
    "for line in text_full.split(\"\\n\"):\n",
    "    for word in line.split(\" \"):\n",
    "        vocab.append(word)\n",
    "        \n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43714\n"
     ]
    }
   ],
   "source": [
    "# Create a set for the vocab\n",
    "vocab = set(vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict mapping vocab to index\n",
    "vocab_to_id = {}\n",
    "id_to_vocab = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    vocab_to_id[word] = index\n",
    "    id_to_vocab[index] = word\n",
    "    \n",
    "# Save the objects\n",
    "with open('vocab_to_id.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab_to_id, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "with open('id_to_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(id_to_vocab, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "with open('vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Create an empty matrix of zeroes\n",
    "tf = np.zeros((len(vocab), len(vocab)))\n",
    "print(type(tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill up the tf matrix\n",
    "for line in text_full.split(\"\\n\"):\n",
    "    words = line.split(\" \")\n",
    "    for i in range(len(words) - 1):\n",
    "        id1 = vocab_to_id[words[i]]\n",
    "        id2 = vocab_to_id[words[i + 1]]\n",
    "        tf[id1, id2] += 1\n",
    "del text_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15287310480\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "print(sys.getsizeof(tf))\n",
    "mat = sp.coo_matrix(tf)\n",
    "print(sys.getsizeof(mat))\n",
    "del tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176561\n",
      "4926\n",
      "37980\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(mat.data))\n",
    "print(mat.row[37577])\n",
    "print(mat.col[37577])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the son\n",
      "son of\n",
      "on the\n",
      "said unto\n",
      "thank you\n",
      "will be\n",
      "of israel\n",
      "out of\n",
      "unto the\n",
      "and they\n",
      "all the\n",
      "shall be\n",
      "for the\n",
      "i will\n",
      "and he\n",
      "to the\n",
      "in the\n",
      "and the\n",
      "the lord\n",
      "of the\n"
     ]
    }
   ],
   "source": [
    "# Find the k most common word pairings\n",
    "k = 20\n",
    "elems = mat.data.copy()\n",
    "elems.sort()\n",
    "for elem in elems[-k:]:\n",
    "    loc = np.where(mat.data == elem)\n",
    "    id1 = mat.row[loc[0][0]]\n",
    "    id2 = mat.col[loc[0][0]]\n",
    "    print(str(id_to_vocab[id1]) + \" \" + id_to_vocab[id2] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common consectuive pair of words, with 13245 instances, was \"of the\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the matrix\n",
    "mat_norm = normalize(mat, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the normalized matrix\n",
    "sp.save_npz(\"norm_matrix.npz\", mat_norm)\n",
    "del mat, mat_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually make random sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back in the matrix and dictionary objects\n",
    "mat_norm = sp.load_npz(\"norm_matrix.npz\")\n",
    "with open('vocab_to_id.pkl', 'rb') as f:\n",
    "    vocab_to_id = pickle.load(f)\n",
    "    f.close()\n",
    "with open('id_to_vocab.pkl', 'rb') as f:\n",
    "    id_to_vocab = pickle.load(f)\n",
    "    f.close()\n",
    "with open('vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hillary is for his cupbearers and asahel and advertising desperate endline\n"
     ]
    }
   ],
   "source": [
    "start_word = \"hillary\"\n",
    "if start_word not in vocab:\n",
    "    print(\"Invalid start word\")\n",
    "\n",
    "sentence = start_word\n",
    "while start_word != \"endline\":\n",
    "    row_ind = vocab_to_id[start_word]\n",
    "    prob_dist = np.array(mat_norm.getrow(row_ind).todense())[0]\n",
    "    next_ind = np.random.choice(range(len(vocab)), p=prob_dist)\n",
    "    start_word = id_to_vocab[next_ind]\n",
    "    sentence += \" \" + start_word\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### start word: 'the'\n",
    "\n",
    "* the nations shall many women working on the people when israel which are known loser who hath sent benaiah the white as i hope of macedonia and invent to the appearance they not nor taking over theaters this is mccarthyism\n",
    "* the original costume was susan berry\n",
    "* the midst of seventy years agothis is mad sometimes referred to be long massive tax just named eutychus being stubborn can call it\n",
    "* the bar them that thou with his works nay but you have sinned against the lord\n",
    "\n",
    "### start word: 'who'\n",
    "* who escaped alone and treasuries shelemiah shemariah and moab and this country is the one came up he defrauded us is through phenice and thou shalt not hearken unto aaron the obamacare\n",
    "* who was a son of them thou shalt thou art not the word again despite obamas terrible\n",
    "\n",
    "### start word: \"hillary\"\n",
    "* hillary lying vanities but the goodness thou shalt hearken ye that were in all this day in twelve hours left to go way but of the lord of israel and her head\n",
    "* hillary off my desire to trump in her shall surely pay attentionand good night and my mother has zero hours of the lord sent ishmael the stranger for the iconic old article in nyc deal w robot rubio sat there amp me and have our hand and the media panel we need a lot of the apprentice no more interesting poll thank you tomorrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Adding a \"memory\" component to the analysis\n",
    "\n",
    "We are going to increase the look back of our analysis to include not a single word, but the last 3 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am the three will vote was able to shur and never have consulted with will also have to comprehend\n"
     ]
    }
   ],
   "source": [
    "# Specify the starting sentence\n",
    "start_words = \"i am the\".split(\" \")\n",
    "\n",
    "# Specify the choice of weigts\n",
    "weight_choice = \"random\"\n",
    "gamma = 0.5\n",
    "if weight_choice == \"uniform\":\n",
    "    weights = np.zeros(len(start_words)) + 1\n",
    "    weights = weights / len(start_words)\n",
    "elif weight_choice == \"decaying\":\n",
    "    weights = [0] * len(start_words)\n",
    "    for i in range(len(start_words)):\n",
    "        weights[i] = math.pow(gamma, i)\n",
    "\n",
    "    weights.reverse()\n",
    "    weights = np.array(weights)/sum(weights)\n",
    "else:\n",
    "    weights = np.array(random.sample(range(0,100), len(start_words)))\n",
    "    weights = weights / sum(weights)\n",
    "\n",
    "for word in start_words:\n",
    "    if word not in vocab:\n",
    "        print(\"Invalid vocab word\")\n",
    "     \n",
    "current_word = start_words[-1]\n",
    "\n",
    "# Number of steps to look back\n",
    "lookback = len(start_words)\n",
    "\n",
    "while current_word != \"endline\":\n",
    "    # Get the last \"n\" words, where \"n\" is the lookback amount\n",
    "    lookback_words = start_words[-(lookback):]\n",
    "    \n",
    "    # Start with the furthest back word and use it's distribution as the start\n",
    "    row_ind = vocab_to_id[start_words[-len(start_words)]]\n",
    "    prob_dist = weights[0] * np.array(mat_norm.getrow(row_ind).todense())[0]\n",
    "    \n",
    "    # For all the rest of the words, add the weighted probability distribution to the result\n",
    "    for i, word in enumerate(lookback_words[1:]):\n",
    "        row_ind = vocab_to_id[word]\n",
    "        prob_dist += weights[i + 1] * np.array(mat_norm.getrow(row_ind).todense())[0]\n",
    "    \n",
    "    # Make sure we don't have repeats\n",
    "    current_word = start_words[-1]\n",
    "    while current_word in lookback_words:\n",
    "        next_ind = np.random.choice(range(len(vocab)), p=prob_dist)\n",
    "        current_word = id_to_vocab[next_ind]\n",
    "    \n",
    "    # Append the predicted word the results\n",
    "    start_words.append(current_word)\n",
    "    \n",
    "print(\" \".join(start_words[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### start words \"donald trump is\":\n",
    "\n",
    "#### weight: decaying\n",
    "\n",
    "* donald trump is thinking an big guy leads to finally someone realize you for years the holy one that o trump house thou shalt j trump jeb used the wilderness a trump you believe\n",
    "\n",
    "* donald trump is u the re tax their credit trump for\n",
    "\n",
    "#### weight: uniform\n",
    "\n",
    "* donald trump is at the trump president j jimmy trumps son\n",
    "\n",
    "* donald trump is we are ye might j trump when ill be trump only run the trump house youll j trump you your words money trump no shepherd he of the your arguing enemies shall be take great thx\n",
    "\n",
    "#### weight: random\n",
    "\n",
    "* donald trump is have you i run donald trump international unlike anything worth trump\n",
    "\n",
    "* donald trump is now hear the children pot of trump for\n",
    "\n",
    "### \"thou shalt not\":\n",
    "\n",
    "#### weight: decaying\n",
    "\n",
    "* though shalt not bless the king hand of illegals to bring ye another will draw\n",
    "\n",
    "* though shalt not sincerely increase supposing the world they say again thou being wilt twelve men they cause some i of am the best interviews and delaying his the bramble and abigail the nabals wife looked bare all my lips soul\n",
    "\n",
    "#### weight: uniform\n",
    "\n",
    "* though shalt not i in will he i is want to big we i too he turned a their way kings counsel\n",
    "\n",
    "* though shalt not save thou shalt gleaned he they that thou me art and i will cast they sank i the have jesus remained u debate would have love they the every tea party i had prayed brought my he i that despiseth atlantic thou make livest these things ten list the on foxandfriends this great i feel say the i will save bless alive thee he the rested themselves what in greensboro your north it country charles he missouri hath trump you they a lacked mother opportunity in any case i disgraceful always we on actually the prophets and im love from\n",
    "\n",
    "#### weight: random\n",
    "\n",
    "* though shalt not go use twitter trump\n",
    "\n",
    "* though shalt not he then let us the freeing super up tuesday morning on television a it is not enough repeal signatures fools big league crowd thou shalt love roast with and an i emotional apprenticenbc still her daughters times fools in feedeth thy among thorns all thy the matters who dennis opposes nd choice worst ye thing say who unto them absalom\n",
    "\n",
    "### \"i am the\":\n",
    "\n",
    "#### weight: decaying\n",
    "\n",
    "* i am the heavenly have forsaken covenanted with no a samaritan as that trust forsake ye begin to the ships asher and they went in out time gave an iraq will it appoint is you trump wins will written heal like neither can could only in truth not their healthcare would plan will be totally proved did are change bring back will i need your to hand pretending of not spare them look and thou he wonder of where to hear used the win will went and he testifieth thou disquieted shalt win\n",
    "\n",
    "* i am the have young havent did was being offered merciful and man feareth have god and with thee o hosted by andrewejohnson\n",
    "\n",
    "#### weight: uniform\n",
    "\n",
    "* i am the ephraimites best which was great\n",
    "\n",
    "* i am the never have smote i them could not try cannot to get trump it am persuaded the law kindred for yourselves therefore said thus love congratulate have him a full of would a prenup\n",
    "\n",
    "#### weight: random\n",
    "\n",
    "* i am the pray will say rebuke am will might say think always made for sat follow gave wonder will love have know just shall was see have just scatter will say looked gave have i said am testified therefore let have unto am may understanding hope have will hate say said daughter shall did have wish love cant let that say go may take am will should dont wrote rejoice will am sent thank will delivered came run should say will am tell his pray saw am will pray know cannot am have had love appointed was will agree find pray go loved completely am agree said had might truly see send will am watched have know am wonder cut will want have shall again would tell will vote need play will darius totally speak may beheld them hope will am\n",
    "\n",
    "* i am the three will vote was able to shur and never have consulted with will also have to comprehend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
